# MSA 기반의 클라우드 티켓팅 플랫폼 개발

[**최종 보고서 전문 보기**](https://github.com/HENOZIWD/PNU-Capstone-2024/blob/main/docs/01.%EB%B3%B4%EA%B3%A0%EC%84%9C/03_%EC%B5%9C%EC%A2%85%EB%B3%B4%EA%B3%A0%EC%84%9C.pdf)

### 마이크로서비스 클라이언트 레포지토리 목록

- [**FE-Main**](https://github.com/clove-pnu/FE-Main) - 각 애플리케이션 모듈들을 가져와 사용하는 호스트 애플리케이션
- [**FE-Auth**](https://github.com/clove-pnu/FE-Auth) - 사용자 인증
- [**FE-Deploy**](https://github.com/clove-pnu/FE-Deploy) - 공연 배포
- [**FE-Monitor**](https://github.com/clove-pnu/FE-Monitor) - 공연 모니터링
- [**FE-My-Ticket**](https://github.com/clove-pnu/FE-My-Ticket) - 예매한 티켓 조회
- [**FE-Ticket**](https://github.com/clove-pnu/FE-Ticket) - 공연 예매
- [**FE-Template**](https://github.com/clove-pnu/FE-Template) - 공연 배포 템플릿 관리

## 🎥 소개 및 시연 영상

> https://www.youtube.com/watch?v=EWoIG3pVcTQ

## 👥 구성원 별 역할

| **이름**  |  **역할**                                       |
|---------|----------------------------------------------|
| **이동현** | **1. 클라이언트 개발**<br> - 마이크로프론트엔드 아키텍처 기반의 클라이언트 시스템 개발<br>**2. 마이크로서비스 개발**<br> - 모니터링 시스템 개발: Prometheus API 활용한 리소스 조회<br> - 모니터링 정보 가시화: JavaScript 그래픽 라이브러리를 통한 자체 구현<br> - 마이크로서비스 통합 테스트<br>**3. 마이크로서비스 아키텍처 안정성 테스트**<br> - 부하 테스트를 통한 서비스 독립성 검증 |
| **강찬석** | **1. 클라우드 인프라 구축 및 관리**<br> - 가상머신, 쿠버네티스 등 인프라 및 네트워크 환경 구축<br>**2. 시스템 관리 마이크로서비스 개발**<br> - 컨테이너 풀 관리 서버 개발: 도커 이미지 관리<br> - 템플릿 관리 서버 개발: 템플릿 생성 및 관리<br> - 시스템 배포 관리 서버 개발: 컨테이너 자동 배포 및 관리<br>**3. 서비스 배포 시간 테스트**<br> - 자동 배포를 통한 서비스 배포 시간 측정 |
| **조용진** | **1. 사용자 인증 서버 개발**<br> - 대칭 키 기반 JWT 토큰: Access, refresh 토큰 발급<br> - 토큰을 이용한 사용자 검증, 토큰 분할을 통한 탈취 방어<br>**2. 공연 마이크로서비스 개발**<br> - 공연 관리 서버 개발: 정보 등록, 수정 등 주요 기능 개발<br> - 티켓 예매 서버 개발: 예매, 예매 이력 조회 등 주요 기능 개발<br>**3. 티켓 판매자 요구사항에 따른 마이크로서비스 추가 개발**<br> - 좌석, 상품 및 결제 유무에 따라 마이크로서비스 개발 |

## 1. 연구 배경
마이크로서비스 아키텍처(MSA)는 시스템을 소규모의 독립적인 서비스들로 분리하고, 각 서비스는 모듈 또는 프로젝트 단위로 나누어 개발 및 관리를 진행하는 소프트웨어 아키텍처이다. 마이크로서비스 아키텍처는 개별 구성 요소 간의 결합도와 기능 간 의존성을 낮추어 시스템 확장성, 유지보수성을 높인다는 특징이 있다.

마이크로서비스 아키텍처에서 개별 마이크로서비스는 약 결합(Loosely Coupling)을 활용하여 연결된다. 약 결합을 통해 연결된 마이크로서비스는 의존성이 낮아 일부 마이크로서비스에서 장애가 발생하더라도 전체 시스템에 큰 영향을 미치지 않는다. 이는, 정확하게 장애가 발생한 마이크로서비스에 대한 복구를 수행할 수 있게 하며, 최종적으로 시스템 회복 지연 시간을 줄일 수 있다. 또한, 개별 마이크로서비스가 기능적으로 완성되어 있으므로, 기존 시스템의 변경 없이 독립적인 마이크로서비스를 개발하여 추가할 수 있는 확장적 측면의 장점도 갖는다.  

개발 시스템에서 결과물을 예측하기 위해서는 레퍼런스로 동작하는 모델이 필요하다. 그러나, 마이크로서비스 아키텍처를 제공하는 표준적인 모델은 존재하지 않으며, 표준 모델을 제공하기에는 다양한 도메인의 모든 서비스 요구사항을 만족시키기 어렵다. 따라서, 특정 도메인에서 마이크로서비스 아키텍처를 제공할 수 있는 기반 구조가 필요하다. 또한, 마이크로서비스 아키텍처는 다수의 서비스가 약 결합을 통해 완성되는 시스템으로 개별 마이크로서비스는 독립적으로 완성된 기능을 가져야 한다. 그러나, 기 개발된 시스템들의 요구사항을 만족시키면서 기능을 분리한 마이크로서비스로 구축하는 것은 시스템의 환경과 도메인 요구사항을 결합시키는 복잡한 개발 및 의사결정 단계가 필요하다.

따라서, 본 과제에서는 티켓 예매 시스템을 활용하는 티켓 판매자/티켓 구매자/시스템 관리자 입장의 요구사항을 반영하는 마이크로서비스 구축 및 배포 시스템을 제안한다. 제안하는 시스템은 티켓 예매 시스템에 필요한 기능 분석을 통해 티켓 예매 도메인에서 마이크로서비스가 동작 가능한 환경 및 기반 구조를 제시한다. 또한, 티켓 예매 시스템의 마이크로서비스 풀을 사전 구축하여 기본 흐름을 수행할 수 있게 함과 동시에, 개별 기능들의 템플릿을 제공하는 것으로 티켓 예매에 필요한 각 이해관계자들의 요구사항이 반영될 수 있도록 구축하였다. 

## 2. 연구 목표
- **컨테이너 풀을 활용한 마이크로서비스 배포 모델 제안**
  - 티켓 판매 시스템을 효율적으로 구축하기 위한 사전 준비된 컨테이너 풀의 활용 방안을 탐색하고, 이를 통해 마이크로서비스를 신속히 배포할 수 있는 시스템을 개발한다.
- **사용자 요구사항을 반영한 맞춤형 마이크로서비스 배포 체계 구축**
  - 시스템 구성에 필요한 마이크로서비스들을 정의한 템플릿을 제안하며, 템플릿 기반 서비스 배포 체계를 구축한다.
  - 템플릿 선택 및 정보 입력을 통해, 사용자 요구사항을 반영하는 서비스 배포 체계를 구축한다.
- **자동화된 서비스 배포 체계 구축**
  - 추가적인 인력 개입 없이, 사용자의 요청에 따라 시스템이 자동으로 서비스를 배포할 수 있는 배포 체계를 구축한다.
- **독립적인 서비스 관리로 인한 시스템 안전성 향상**
  - 사용자별, 기능별 독립적인 마이크로서비스를 배포하여, 기능적 독립성을 통해 더욱 견고한 시스템을 구축한다. 
  - 각 서비스가 다른 서비스에 영향을 미치지 않도록 하여, 장애 발생 시 신속한 복구와 대응이 가능한 시스템 구조를 설계한다. 

## 3. 시스템 환경 구축 및 세부 구현

### 3.1. VM 인스턴스

>  GCP 상에 구축한 VM 인스턴스 목록

<img width="1280" alt="GCP 상에 구축한 VM 인스턴스 목록" src="https://github.com/user-attachments/assets/e9ca0fb8-e921-43fb-b7c3-5177c7a4c51b" />

본 과제의 시스템을 구축하기 위한 인프라 환경으로 GCP에서 제공하는 가상머신 환경을 이용하였다. 그림과 같이 쿠버네티스 노드의 역할에 따라 클러스터 노드 1개, 워커 노드를 2개 배치하여 환경을 구성하였다. 

### 3.2. 게이트웨이 API

> 사용자가 시스템에 요청을 보낼 때 해당 트래픽이 각 서비스로 전달되는 과정

<img width="1280" alt="사용자가 시스템에 요청을 보낼 때 해당 트래픽이 각 서비스로 전달되는 과정" src="https://github.com/user-attachments/assets/85c794b1-5e42-4171-9fe4-e78615216330" />

각 마이크로서비스의 원활한 통신을 위해 게이트웨이 API를 사용하였다. 시스템으로 전달되는 모든 트래픽은 cluster 노드로 전달된다. Cluster 노드에는 베어메탈 환경에서 로드밸런서의 역할을 하는 MetalLB를 배치하여 외부 트래픽이 클러스터로 들어올 수 있게 별도의 외부 IP를 할당하였다. 사용자 요청은 Cluster 노드에 배치된 NGINX를 통해 MetalLB의 외부 IP로 연결된다.

MetalLB를 통해 클러스터로 유입된 트래픽은 Kubernetes Gateway API로 전달된다. Gateway API는 HTTP(S) 트래픽의 목적지 경로(path)에 따라 요청을 적절한 서비스로 라우팅해주는 역할을 수행한다. Gateway와 HTTPRoute 리소스 정의를 바탕으로 트래픽 경로와 설정된 규칙에 따라 각 요청을 실제 수행하기 위한 마이크로서비스 컨테이너에 요청을 전달한다. Kubernetes Gateway API의 적용을 위해 Istio Ingress Gateway를 사용하였다.

### 3.3. 컨테이너 배포 관리

> 공연 배포 시 생성된 namespace

<img width="1280" alt="공연 배포 시 생성된 namespace" src="https://github.com/user-attachments/assets/19f842aa-c577-4421-a36c-8d7feb011e21" />

> 공연 배포를 통해 namespace에 컨테이너가 배포된 예시

<img width="1280" alt="공연 배포를 통해 컨테이너가 배포된 예시" src="https://github.com/user-attachments/assets/3fff30d7-ca11-408b-a91e-e16c262a8368" />

> 공연 배포 시 생성된 namespace에 접근하기 위한 token 조회

<img width="1280" alt="공연 배포 시 생성된 namespace에 접근하기 위한 token 조회" src="https://github.com/user-attachments/assets/b3512323-40e3-4d37-9372-6cac1d9908b8" />

사용자 요청에 따라 컨테이너를 배포하는 작업을 수행하기 위해 컨테이너 배포 관리 마이크로서비스를 배치하였다. 배포 관리 서버는 공연 마이크로서비스 배포에 관련된 CRUD 작업을 수행한다.

배포 시, 마이크로서비스의 관리 및 권한 제어를 위해 쿠버네티스의 RBAC 기반의 접근제어 기법을 활용하였다. 사용자의 배포 요청 시, 공연 별 namespace가 생성되고, 해당 namespace에 접근 권한을 가지는 토큰을 생성한다. 생성된 토큰은 서버에서 사용자 정보에 매핑하여 관리하기 때문에, 사용자는 별도의 토큰 관리 없이 자신이 배포한 리소스들에 대한 접근이 가능하다.

### 3.4. 모니터링 시스템

시스템의 클라우드 환경 및 컨테이너 상태를 확인하기 위해 모니터링 시스템을 구축하였다. 모니터링 시스템은 배포한 각 컨테이너들의 리소스 사용량을 수집 및 제공하는 기능을 수행한다.

모니터링 시스템은 cAdvisor를 활용하여 컨테이너들의 리소스 사용량을 수집하고 Prometheus에 저장한다. Prometheus는 저장된 리소스 사용량 정보를 제공하는 Prometheus API를 제공한다. 클라이언트는 Prometheus API를 통해 리소스 사용량 정보를 조회할 수 있다.

> Prometheus API 요청 예시(컨테이너 CPU 사용량 조회)

<img width="1280" alt="Prometheus API 요청 예시(컨테이너 CPU 사용량 조회)" src="https://github.com/user-attachments/assets/4788d42e-a132-4435-9ce4-b13922a50f93" />

파라미터를 통해 쿼리와 조회 기간, 값 사이의 시간 간격을 입력하여 API를 요청한다. API 요청 결과에서 시간과 리소스 값 쌍으로 데이터가 반환됨을 확인할 수 있다.

> 공연 서버 모니터링 대시보드

<img width="1280" alt="공연 서버 모니터링 대시보드" src="https://github.com/user-attachments/assets/f86c9091-2119-4ea3-9d3a-0e56d7e5a6aa" />

모니터링 시스템을 통해 공연 모니터링 마이크로서비스에서 공연 판매자가 자신이 배포한 공연 서버의 자원 현황을 손쉽게 파악할 수 있다.

### 3.5. 마이크로프론트엔드

> 마이크로프론트엔드 아키텍처

<img width="1280" alt="마이크로프론트엔드 아키텍처" src="https://github.com/user-attachments/assets/d446918b-e9da-4596-a12b-2a2f8089ab2a" />

마이크로프론트엔드 아키텍처는 마이크로서비스 아키텍처 구성에서 프론트엔드(사용자 뷰, 대시보드 등)를 지원하기 위해 도입된다. 기존 모놀리식 아키텍처에서 활용하는 프론트엔드 시스템은 사용자 인터페이스의 복잡성이 높아, 마이크로서비스 아키텍처에 적용할 경우 병목현상과 같은 문제가 초래할 수 있다.

마이크로프론트엔드 아키텍처는 이러한 문제를 해결하기 위해 복잡한 사용자 인터페이스를 자율적으로 개발, 테스트 및 배포할 수 있는 작고 독립적인 구성 요소인 모듈로 분해한다. 분해한 각 모듈은 독립적이므로 개발 및 테스트, 배포 과정 동안의 시간이 단축되며, 모듈과 전체 시스템, 모듈과 모듈 간의 영향도가 감소하므로 기존 모놀리식 아키텍처에 비해 병목현상과 같은 문제가 완화된다. 또한, 마이크로프론트엔드 아키텍처 그림에서 확인할 수 있듯이 각 모듈을 다른 모듈의 환경에 제약 받지 않고 다양한 형태의 마이크로서비스로 구현이 가능하여 구현 자유도가 높다는 장점이 있다.

> 마이크로프론트엔드 아키텍처를 적용한 클라이언트 애플리케이션 구성도

<img width="1280" alt="마이크로프론트엔드 아키텍처를 적용한 클라이언트 애플리케이션 구성도" src="https://github.com/user-attachments/assets/2bb07c13-8c97-4f5b-8c7e-3da9a7ba9048" />

클라이언트 구성도에서 쿠버네티스에 배포한 각 애플리케이션은 모듈들을 webpack 설정을 통해 expose하여 외부로 노출시킨다. 외부로 노출된 모듈들의 정보들은 각 애플리케이션의 remoteEntry.js에 저장되며, 호스트 애플리케이션에서 remoteEntry.js의 경로로 접근하여 해당 모듈을 가져올 수 있다. 쿠버네티스에 배포된 각 애플리케이션들은 Gateway API를 통해 엔드 포인트가 결정되고, 호스트 애플리케이션에서 해당 경로를 통해 모듈들을 가져와 사용한다.

클라이언트 구성도에서 사용자 인증, 공연 배포, 공연 모니터링, 티켓 관리 기능을 가져와 사용하는 호스트 애플리케이션과, 공연 배포를 통해 배포되어 사용자 인증 기능을 가져와 사용하는 공연 예매 애플리케이션을 확인할 수 있다.

## 4. 연구 결과 분석

### 4.1. 시스템 안정성 평가

> 공연 마이크로서비스 부하 테스트

<img width="1280" alt="공연 마이크로서비스 부하 테스트" src="https://github.com/user-attachments/assets/75a481dd-b3a2-417a-9e25-44179e6730e0" />

배포한 공연 마이크로서비스 간의 독립성을 테스트하기 위해 서로 다른 namespace에 배포된 공연 마이크로서비스를 대상으로 부하 테스트를 진행하였다.

테스트 과정은 사전에 festival namespace와 festival2 namespace에 각각 공연을 배포한 뒤, Python 라이브러리를 이용하여 테스트 프로그램을 구성하고 두 공연 마이크로서비스 중 festival namespace에 배포된 마이크로서비스에만 약 1분간 API 요청을 보내 부하를 가하는 방식으로 진행하였다. 결과 도출을 위해 부하 테스트 시작과 동시에 리눅스 쉘 스크립트를 통해 kubectl top pods 명령어를 두 공연의 namespace에 대해 각각 실행하여 CPU 사용량을 수집하였다.

그 결과 festival namespace에 배포된 마이크로서비스의 CPU 사용량이 증가하는 반면, festival2 namespace에 배포된 마이크로서비스의 CPU 사용량은 부하 테스트와 관계없이 낮은 사용량을 유지하는 것을 확인할 수 있었다. 따라서 본 과제에서 구현한 시스템에서 배포된 공연 마이크로서비스 간의 독립성을 확인하였다.

기존 모놀리식 시스템의 문제점 중 시스템 안정성의 부족을, 본 과제에서는 마이크로서비스 아키텍처를 통해 효과적으로 해결할 수 있었다. 각 기능을 독립적으로 분리하여 운영하면서, 특정 서비스에 장애가 발생하더라도 전체 시스템이 영향을 받지 않도록 설계하였다. 이는 장애가 발생하더라도 특정 기능에 대한 복구만을 수행하여 빠른 복구를 가능하게 하였고, 시스템의 안정성과 유지보수성을 확보할 수 있었다.

### 4.2. 시스템 성능 평가

기존 티켓팅 시스템의 문제점 중 하나였던 사용자 요구사항의 반영이 어렵다는 점을 템플릿 기반 마이크로서비스 배포 체계를 구축하여 해결하였다.

사용자는 사전 정의된 다양한 템플릿을 선택할 수 있고 필요한 정보를 입력함으로써, 각 사용자 별 맞춤형 서비스 배포가 가능하게 하였다. 이러한 템플릿 기반 접근 방식을 통해 서비스 배포의 유연성과 신속성을 크게 향상시켰다. 또한 컨테이너 풀을 활용한 배포 모델과 자동화된 배포 지원 시스템을 통해 신속한 서비스 배포가 가능하게 하였다.

> 공연 배포 시간 테스트 결과(100회 평균)

<img width="1280" alt="공연 배포 시간 테스트 결과(100회 평균)" src="https://github.com/user-attachments/assets/e4f800c8-636e-4316-b52d-1e97e19a4fd3" />

시스템의 성능을 평가하기 위해 3개의 마이크로서비스가 포함된 템플릿을 배포했을 때 서비스 등록이 완료되기 까지의 시간을 측정하였다. 총 9개의 리소스가 쿠버네티스에 등록되고 동일한 테스트를 100회 반복하여 평균 소요 시간을 계산하였다.

사용자 요청 처리 및 쿠버네티스 상에 리소스가 등록되는 과정은 약 0.5초 이내로 수행된다. 대부분의 시간이 네트워크 지연 시간으로 클라이언트 위치에 따라 달라질 수 있다. 컨테이너 준비 시간은 쿠버네티스 리소스가 등록되었을 때, 컨테이너 풀에서 이미지를 pull 하는 과정과 컨테이너를 구동하는 시간이 포함되어 있으며 약 5초 이내로 수행되는 것을 확인하였다.

이후 각 컨테이너에서 구동되는 서버 애플리케이션은 애플리케이션 특성에 따라 상이하다. 본 시스템에서는 Spring, FastAPI 기반의 서버를 구축하였고, Spring 서버의 경우 약 30초, FastAPI 서버의 경우 약 1초의 서버 구동 시간을 필요로 했다.

결론적으로, 사용자 요청이 발생한 시점부터 서비스가 배포되기까지 FastAPI 서버의 경우 평균 6초, Spring 서버의 경우 36초 정도의 시간이 소요되는 것을 확인했다. 기존에 티켓팅 시스템에서 여러 복합적인 이유로 공연 등록 요청 처리 기간이 평균 3~4일 소요되는데 비해, 본 시스템에서는 사용자 요청 반영을 즉각적으로 반영할 수 있는 시스템을 구축하였다.
